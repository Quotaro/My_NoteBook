# PCネットワーク（WEB）、インフラ関連

## 目次 [Contents]

1. [概要](#ID_1)
1. [インフラの基礎事項](#ID_3)
    1. [システム基盤 [System Infrastructure]](#ID_3-1)
    1. [オンプレミスとクラウド](#ID_3-2)
		1. [オンプレミス [on-premise]](#ID_3-2-1)
		1. [パブリッククラウド [public cloud]](#ID_3-2-2)
		1. [プライベートクラウド [private cloud]](#ID_3-2-3)
		1. [クラウドとオンプレミスの選択](#ID_3-2-4)
	1. [ハードウェアとネットワークの基礎事項](#ID_3-3)
    	1. [ネットワーク機器](#ID_3-3-1)
		1. [OSI 基本参照モデル](#ID_3-3-2)
		1. [ネットワークアドレス](#ID_3-3-3)
		1. [リピーター、ハブ](#ID_3-3-4)
		1. [ブリッジ、レイヤー２スイッチ](#ID_3-3-5)
		1. [ルーター、レイヤー３スイッチ](#ID_3-3-6)
		1. [ファイアウォール](#ID_3-3-7)
1. [Linux OS の基礎事項](#ID_4)
	1. [Linux カーネル](#ID_4-1)
	1. [Linux のファイルシステム](#ID_4-2)
	1. [Linux のディレクトリ構成](#ID_4-3)
	1. [Linux のセキュリティ](#ID_4-4)
1. [ミドルウェアの基礎事項](#ID_5)
	1. [Web サーバーのミドルウェア](#ID_5-1)
	1. [データベースサーバーのミドルウェア](#ID_5-2)
	1. [システム監視ツールのミドルウェア](#ID_5-3)
1. [インフラの構成管理の基礎事項](#ID_6)
	1. [コードによるインフラの構成管理](#ID_6-1)
	1. [継続的インテグレーション、継続的デリバリー](#ID_6-2)
1. [システム基盤の構築、運用の流れ（ウォータフォール開発、アジャイル開発）](#ID_7)
1. [Docker](#ID_8)
	1. [Docker のコンテナ技術](#ID_8-1)
	1. [プログラマー視点からの Docker](#ID_8-2)
	1. [Docker の機能](#ID_8-3)
		1. [Bulid : Docker イメージを作る機能](#ID_8-3-1)
		1. [Ship : Docker イメージを共有する機能](#ID_8-3-2)
		1. [Run : Docker コンテナを動かす機能](#ID_8-3-3)
	1. [Docker を構成するコンポーネント](#ID_8-4)
		1. [Docker Engine（Docker のコア機能）](#ID_8-4-1)
		1. [Docker Registry（イメージ公開・共有）](#ID_8-4-2)
		1. [Docker Compose（複数コンテナ一元管理）](#ID_8-4-3)
		1. [Docker Machine（Docker 実行環境構築）](#ID_8-4-4)
		1. [Docker Swarm（クラスタ管理）](#ID_8-4-5)
	1. [Docker のコア機能が動く仕組み](#ID_8-5)
		1. [コンテナを区画化する仕組み（namespace）](#ID_8-5-1)
		1. [リソース管理の仕組み（cgroups）](#ID_8-5-2)
		1. [ネットワーク構成（仮想ブリッジ・仮想 NIC）](#ID_8-5-3)
		1. [Docker イメージのデータ管理の仕組み](#ID_8-5-4)
	1. [xxx](#ID_8-x)

---

<a id="ID_1"></a>

## ■ 概要
![image](https://user-images.githubusercontent.com/25688193/41355764-39f414c0-6f5d-11e8-9991-efdc8657b344.png)<br>

開発したアプリケーションをリリースして、エンドユーザーに使いやすく利用してもらうためには、ハードウェア、OS 等のインフラから構成される **システム基盤 [System Infrastructure]** を構成し、その層の上にアプリケーションの実行環境を構成する必要がある。（上図）<br>
**Docker は、このアプリケーションの実行環境を作成、管理するためのプラットホーム [platform] （土台環境）である。**<br>

<br>

従来のウォータフォールモデルで開発されるシステム開発では、アプリケーションの実行環境の構築は、ハードウェアやネットワークに精通したインフラエンジニアが行い、アプリケーション本体の開発は、アプリケーションエンジニアが行う手法が一般的であった。

しかしながら、クラウド技術の登場により、この従来のシステム開発の手法が大きな変化があった。<br>
具体的には、従来のように自社でデータセンターやマシンルームを保有するような、**オンプレミス [on-premises]** 環境で駆動させていたサーバー群を、クラウド上の**仮想インスタンス [VM : Virtual Machine instance]** に移行し、又、クラウドサービスにあるデータベースやネットワークを利用することで、
アプリケーションの実行環境の構築範囲が極めて小さくなり、短いサイクルで（アプリケーションの）リリースを繰り返すスタイルに変化した。<br>

クラウドを構成する技術の多くは、１台の **物理ホスト** （ネットワーク上のサーバーの一種）上で駆動するシステムとは異なり、**分散環境 [distributed environment]** での駆動が基本となる。<br>
そして、このような分散環境では、従来のインフラエンジニアによる手動の操作ではなく、自動化ツールを使用して**オーケストレーション**（複雑なコンピュータシステムの自動化）を行う。<br>
その為、インフラエンジニアには、インフラ技術に加えてアプリケーションエンジニアと同様に、コードを各スキルが求められるようになった。<br>
一方で、アプリケーションエンジニアも、これまでインフラエンジニアのタスクであった、プロダクション環境（本番環境）への **デプロイ [deploy]**（主にWEBアプリケーションなどのシステム開発においてシステムを利用可能な状態にすること）やテストなども、アプリケーションエンジニア自らが行うことも可能になったため、OS（カーネル）やネットワークのインフラ技術の基礎知識が必要になった。<br>

---

<a id="ID_3"></a>

## ■ インフラの基礎事項

<a id="ID_3-1"></a>

### ◎ システム基盤 [System Infrastructure]
システム基盤とは、アプリケーションを駆動させるために必要なハードウェアや OS、ミドルウェアなどのインフラのことを指す。このシステム基盤は、以下の基本要素から構成される。

![image](https://user-images.githubusercontent.com/25688193/41374963-98d188d8-6f8f-11e8-8b8f-64e6a2c2718c.png)<br>

- ハードウェア<br>
	システム基盤を構成する物理的な要素で、具体的には、サーバー機材本体やデータを保管するためのストレージ、電源装置を指す。<br>
	広義には、これらハードウェア群を設置するデータセンターの設備（建屋、空調、セキュリティ設備、消化設備）を含む。

- ネットワーク<br>
	システムの利用者が遠距離からアクセスできるように、サーバー群を接続する装置。<br>
	具体的には、ルーター、スイッチ、ファイアウォール等のネットワーク機器や、それらを接続するためのケーブル配線など。無線LAN で接続する場合は、アクセスポイントも必要となる。

- OS<br>
	クライアントOSとしては、Windows, masOS等がある。<br>
	サーバーOSとしては、Linux, Unix, Windows Server 等がある。

- ミドルウェア<br>
    ここでいうミドルウェアは、”サーバーOS上で”サーバーが特定の役割を果たすための機能をもつソフトウェアを指す。

<br>

<a id="ID_3-2"></a>

### ◎ オンプレミスとクラウド

<a id="ID_3-2-1"></a>

#### ☆ オンプレミス [on-premise]
自社でデータセンターを保有して、システム構築から運営までを行う形態。<br>
システム基盤の構成要素であるサーバーやネットワーク機材を自らが購入＆調達し、システム要件に応じてインフラを構築し、自社（又は関連子会社）で運用を行う。<br>
又、ハードウェアだけでなく、OSやミドルウェアも全て自社で購入し、ライセンス管理やバージョンアップも自前で行う。<br>

そのため、初期のシステム構築に多額の費用がかかる。<br>
更に、システム稼働後の運用にかかる費用も、”システムの使用量にかかわらず”一定額を負担しなくてはならないのが特徴である。<br>

<a id="ID_3-2-2"></a>

#### ☆　パブリッククラウド [public cloud]
![image](https://user-images.githubusercontent.com/25688193/41385096-7c622ebc-6fb4-11e8-9864-8a90ddd56d2e.png)<br>
インタネットを介して、”不特定多数に提供される”サービス。<br>
自社でデータセンターを保有しなくて済むので、サーバーやネットワークなどのインフラに関する初期費用が不要となる。<br>
提供するサービスにより、**IaaS（イアース、アイアース） / PaaS（パース） / SaaS（サース）** などが存在する。<br>

![image](https://user-images.githubusercontent.com/25688193/41385123-a7c5b402-6fb4-11e8-872b-6fa54237e49a.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41385487-f14e9c5e-6fb6-11e8-8bc6-e53726a39e03.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41385504-0841d976-6fb7-11e8-8025-8592cc21d159.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41385951-b3b6c3f0-6fb9-11e8-997b-b51535142cdd.png)<br>


<a id="ID_3-2-3"></a>

#### ☆ プライベートクラウド [private cloud]
![image](https://user-images.githubusercontent.com/25688193/41386506-989d9ed8-6fbc-11e8-9191-0154b10ece76.png)<br>

特定の企業グループにのみ提供されるクラウドサービス。<br>
利用者が限定されるため、セキュリティが担保しやすく、又、独自の機能やサービスを追加しやすいことが利点として挙げられる。


<a id="ID_3-2-4"></a>

#### ☆ クラウドとオンプレミスの選択

##### クラウドが適しているケース
一般的に、クラウドが向いているシステムには以下のようなケースが挙げられる。<br>

- トラフィックが変動しやすいシステム<br>
	システムには、従業員向けシステム（勤怠管理システム、経理システム等）もあれば、コンシューマ向け（予約システム、オンラインショッピング、オンラインゲームのバックエンド、動画配信等）も存在する。<br>
	前者の従業員向けシステムでは、利用者が限定されるためトラフィックの予想がしやすい。<br>
	一方、コンシューマ向けシステムでは、正確なトラフィックの予想が難しくなる。<br>
	このようなトラフィックの量に応じて、システム基盤のサーバーのスペークやネットワークの帯域を見積もる設計を **”サイジング”** と呼ぶ。サイジングが難しいシステムでは、トラフィックの量に応じてシステムを短期間に容易に増強できるクラウドが向いている。<br>

- 災害対策で日本以外にバックアップを構築したいシステム<br>
- 早くサービスインさせたいシステム<br>

##### オンプレミスが適しているケース

- 高い可用性が求められるシステム<br>
	クラウドでは、システムの可用性（ネットワークの瞬断が許されない等の要件）をクラウドベンダーが保証する。<br>
	従って、クラウドベンダーが保証する以上のシステムの可用性が必要となる場合には、クラウドを本番運用できない。<br>

- 機密性の高いデータを扱うシステム<br>
	データの（ハードウェア上の）保管場所は、クラウドクラウドベンダー側によって決められる。<br>
	従って、物理的な保管場所を明確にするシステムでは、データをパブリッククラウド上に保管することはできないことになる。<br>

- 特殊な要件があるシステム<br>
	特殊な要件、例えば、汎用的でないデバイスや特殊なプラットホーム上でしか動作しないシステム（医療システムなどの専用機器との接続が必要となるシステムなど）を構築、移行する必要がある場合、それらの環境にクラウド側が対応してなければ、クラウドを利用できない。<br>

<br>


<a id="ID_3-3"></a>

### ◎ ハードウェアとネットワークの基礎事項
システム基盤の最下位レイヤーを構成する要素は、ハードウェアとネットワークである。<br>
![image](https://user-images.githubusercontent.com/25688193/41751849-75806898-75fe-11e8-9acb-0aa35793cd1c.png)<br>

ここでは、Docker インフラを構成する際に基本となるハードウェア、ネットワークの基礎事項について見ていく。<br>

<a id="ID_3-3-1"></a>

#### ☆ サーバー機器
一般的なオンプレミスでのシステム基盤は、複数台のサーバー機器から構成される。<br>
クラウドの場合は、仮想マシンのインスタンス（オンプレミスでのサーバー機器に相当）の種類を要件に応じて選定する必要がある。<br>

- CPU<br>
	サーバーで利用される CPU の多くは、コア（CPUの演算回路。コアの数が多いほど同時に演算処理できる数が増加）を複数個もつマルチコアが利用される。<br>
	オンプレミスの場合でもクラウドの場合でも、高性能な CPU や GPU ほど多くの費用がかかる。<br>

- メモリ<br>
	サーバー用のメモリは、省電力でエラー処理が搭載されているものが多く選定させる。<br>
	オンプレミスの場合でもクラウドの場合でも、高性能なメモリほど多くの費用がかかる。<br>

- ストレージ<br>

<br>

<a id="ID_3-3-2"></a>

#### ☆ OSI 基本参照モデル
OSI 基本参照モデルは、ISO によって規定されたコンピューターの通信機能を階層構造に分割した概念モデルであり、通信プロトコルを７つの階層に分けて定義したものである。この階層化により、様々な技術同士の相互接続性を確保することが出来る。<br>
![image](https://user-images.githubusercontent.com/25688193/41754029-87f604c6-760b-11e8-91ad-42ca26fc795a.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41753964-705e7ee2-760b-11e8-94cf-35b3d3beba0e.png)<br>

- 物理層（レイヤー１）<br>
	通信機器の物理的、電気的な特性に関する通信プロトコルを規定する。<br>
	具体的には、ビット列を電気信号に変換するための規定やケーブルやコネクタの形状などを規定する。<br>

- データリンク層（レイヤー２）<br>
	直接的に接続された同じネットワーク内（同一セグメントという）にあるノード間に関しての通信プロトコルを規定する。<br>
	具体的には、どのデバイスにデータ転送（パケット）するのかをネットワークデバイス固有の MAC アドレスで識別した上でデータ転送を行う。<br>
	レイヤー２におけるアドレッシング（デバイスを識別するために ID などの識別子を付随すること）は、
	デバイス固有の MAC アドレスを付随することであるが、より詳細には、宛先 MAC アドレスと送信元 MAC アドレスをフレーム情報として付随する。この際に、宛先 MAC アドレスは、実際に信号を送らないと取得できない。<br>

	尚、MAC アドレスは、実際のデバイスの位置情報を含まないため、インタネット上の場所を特定することはできない。<br>
	従って、後述のネットワーク層で付随される IP アドレスと一緒にしないと、どこにネットワークが存在するのかという情報を得ることはできず、結果として、異なるネットワーク内での通信を構築できない。<br>

- ネットワーク層（レイヤー３）<br>
	異なるネットワーク内にあるノード間に関しての通信プロトコルを規定する。<br>
	具体的には、異なるネットワーク内のデータ転送における経路選択をルーティングというが、このルーティングを IP アドレスをもとに実現する。<br>
	（IP アドレスは、ネットワークデバイスがどこに存在するかの情報をもつため、これを元に異なるネットワーク間の通信を確立できる。）<br>

![image](https://user-images.githubusercontent.com/25688193/41798780-ed455e78-76a9-11e8-9439-694969c69e16.png)<br>

- トランスポート層（レイヤー４）<br>
	ノード間のデータ転送の信頼性（伝送エラー検出等）を確保するための通信プロトコルを規定する。<br>
	より詳細には、確認応答（ACK）、フロー制御（データが溢れないようにバッファに保留する制御）を行い、信頼性の高いデータ転送を実現する。<br>
	
	又、宛先ホストの先にあるホストのどの通信アプリケーション（WEB サーバーを利用した WEB ブラウザやメールサーバーを利用したメールソフトなど）へのデータ転送なのかを判別するために、ヘッダーにポート番号の情報を付随する。<br>
	![image](https://user-images.githubusercontent.com/25688193/41805103-8774186c-76dd-11e8-918f-856ed7f3b470.png)<br>
	このときの通信プロトコルとしては、**TCP** 或いは **UDP** が用いられる。<br>

![image](https://user-images.githubusercontent.com/25688193/41801511-0294ea8a-76b6-11e8-9025-c04286645d7c.png)<br>

- セッション層（レイヤー５）<br>
	通信のコネクション確立のタイミングやデータ転送のタイミングに関する通信プロトコルを規定する。<br>
	尚、ここでいうセッションとは、アプリケーション間での要求（リクエスト）と応答（レスポンス）で構成されるものを表している。<br>

- プレゼンテーション層（レイヤー６）<br>
	データの保存形式や圧縮、文字コードなどのデータの表現形式に関する通信プロトコルを規定する。<br>

- アプリケーション層（レイヤー７）<br>
	Web の HTTP やメール転送の SMTP などのアプリケーション固有の通信プロトコルを規定する。<br>

<br>

OSI 参照モデルでは、各層にそれぞれの通信に関する規定（通信プロトコル）を定めているが、この通信プロトコルに基づいた通信データを作るために、第７層 → 第１層の順で各層の送り主や宛先などのヘッダー情報を入れてカプセル化する。<br>
逆に、送り先のコンピューターは、送られてきたデータを開封し、非カプセル化する。（下図参照）
![image](https://user-images.githubusercontent.com/25688193/41753120-6c16d064-7606-11e8-890a-702cb2c2b042.png)<br>
![image](https://user-images.githubusercontent.com/25688193/41753152-972370a0-7606-11e8-9fcd-edee34af4c39.png)<br>


<a id="ID_3-3-3"></a>

#### ☆ ネットワークアドレス

- MAC アドレス（物理アドレス）<br>
	無線 LAN のチップなどのネットワークデバイスに物理的に割り振られたデバイス固有のアドレス（48 bit）。<br>
	OSI 参照モデルの第２層（データリンク層）で使用される。<br>

	48 bit の内、前半の 24 bit は、部品メーカーを識別する番号で、
	後半の 24 bit は、各部品メーカーが重複しないように割り当てている番号。<br>
	16 進数表記で表示し、先頭から２バイトづつ区切って表記する。<br>

- IP アドレス（論理アドレス）<br>
	インタネットに接続されたコンピューターやネットワークデバイスに割り当られたネットワークに固有の識別番号。<br>
	IP アドレスは、ネットワークデバイスがどこにあるのかという情報を持っており、ネットワークの中でユニークなものでなくてはならないため、アドレスの割り当ては、NIC（ネットワークインフォメーションセンター）という団体によって行われる。<br>
	
	現在広く普及している「IPv4」は、８ビットづつ４つに区切られた３２ビットであり、「192.168.1.1」というように 0 ~ 255 までの１０進数の数字を４つ並べて表記する。<br>


<a id="ID_3-3-4"></a>

#### ☆ リピーター、ハブ
OSI参照モデルの第１層（物理層）に対応するネットワークデバイス。<br>
通信では、実際に回線（銅線、光ファイバー、無線LANなど）を通じてアナログ信号やデジタル信号を送るが、
この信号を送る際に、信号の減衰、ノイズ、衝突などの障害が発生することがある。<br>
これらは、ケーブルを使う際の電気抵抗が直接的な原因で発生するため、
この電気抵抗を減らし、多くの機材を接続することを可能にしたもののが、リピーターやハブというネットワークデバイスである。<br>

- リピーター<br>
	弱まったりノイズがのった信号に対して、増幅や整形をすることで元の同じ形に戻す役割を担う信号増幅器。<br>
	（スイッチやルーターとは異なり、制御は行わず、ただ単に信号を増幅するのみ。）<br>

- ハブ<br>
	リピーターと同様に信号の整形目的での信号の増幅を行い、かつ多くの機材を接続することが出来る装置。<br>


<a id="ID_3-3-5"></a>

#### ☆ ブリッジ、レイヤー２スイッチ
OSI 参照モデルの第２層（データリンク層）に対応するネットワークデバイス。<br>
ネットワーク層では、同じネットワーク内のデバイス間通信を、MACアドレスを付随することで実現するが、イーサネットでは、この際に通信のコリジョンが発生してしまう可能性がある。<br>
この通信のコリジョンを解決するのがブリッジや（レイヤー２）スイッチである。<br>

- ブリッジ<br>
	衝突ドメインのようなネットワークの区切りであるセグメント（言い換えれば、直接的に接続された同じネットワーク）において、**２つのセグメントを繋げるのが、ブリッジの基本的な役割である。**<br>
	又、**MAC アドレスフィルタリング**の役割も持つ。<br>
	**このフィルタリング機能により、衝突ドメインを分割し、結果として、通信トラフィックを低下させることが出来る。**<br>

- （レイヤー２）スイッチ<br>
	基本的には、ブリッジと同様の機能をもつが、異なる点は、マルチポートでスイッチングするという点である。<br>
	詳細には、ブリッジが異なる？ネットワークにフレーム（MACアドレスを付随した通信データ）を送信するかしないかだけを判断するネットワークデバイスであったのに対し、<br>
	**スイッチは、自身が所持しているアドレステーブルを参考にして、各デバイスの送信元と宛先が１対１で接続されている状態にすることが出来る。（マルチポート）**


<a id="ID_3-3-6"></a>

#### ☆ ルーター、レイヤー３スイッチ
OSI 参照モデルの第３層（ネットワーク層）に対応するネットワークデバイス。<br>
２つ以上の異なるネットワーク間の通信を中継し、どのルートを通じてデータ転送を行うかの経路選択（ルーティング）機能を持つ。<br>

より詳細には、ルータ自身の各ポートはそれぞれのネットワークに所属しており、それぞれ固有の MAC アドレスを持っている。そして、ルーティングテーブルと呼ばれる「宛先ネットワーク」、「次の中継ルータ」、「距離」、「送信ポート」という情報をストックしたテーブルを持っており、
このルーティングテーブルを元に目標のノードに対する最適ルートを見つけてルーティングを行う。<br>

ルーティングテーブルを作成する際に、他のネットワークのルートを知っている必要があるが、この方法として、静的ルーティングと動的ルーティングの２種類が存在する。<br>
静的ルーティングは、ネットワーク管理者が手動でルートを入力して経路決定を行う方法である。<br>
一方、動的ルーティングは、ネットワーク上のルーター同士がルート情報を交換し、自動的にルーティングテーブルを作成する。<br>

レイヤー３スイッチは、ルーターとほぼ同じ機能を持つが、ルーティングをハードウェアで実現するため、高速で動作し、接続できるイーサネットのポート数が多いのが異なる点である。<br>


<a id="ID_3-3-7"></a>

#### ☆ ファイアウォール
不要な通信の遮断は、最も有効なセキュリティ対策の１つであるが、**ファイアウォールは、内部ネットワークとその外部との通信を制御することで、内部ネットワークの安全を維持する技術である。**<br>
このファイアウォールには、その遮断方法に応じて幾つかの種類が存在する。<br>

- パケットフィルタ型<br>
	通過する通信データのパケットを、ポート番号や IP アドレスを元に目フィルタリングする方法。<br>
	例えば、「ポート番号 80 (http) と 443 (https) のみ通過してよい」や「安全なセグメント（直接的に接続された同じネットワーク）から届いたパケット以外は全て破棄する」などのルールを決めて、そのルールを元に通信をフィルタリングする。<br>

- アプリケーションゲートウェイ型<br>
	パケットだけではなく、アプリケーションプロトコルのレベルで外部との通信を代替し制御する方法。<br>
	一般的には**プロキシサーバー**と呼ばれている。（プロキシとは、「代理」という意味）

---

<a id="ID_4"></a>

## ■ Linux OS の基礎事項
Linux は、1991 年にフィンランドの Linus Torvalds 氏によって開発された Unix 互換のサーバー OS である。<br>
Linux は、Intel の x86 系マイクロプロセッサを搭載したコンピューターだけでなく、Alpha や SPARC などのプラットホームでも動作する。又、スマートフォンや組み込み機器の OS としても動作している特徴がある。<br>

一般的に、単に Linux と言った場合、以下の２つの意味の何れかで用いられる。<br>
![image](https://user-images.githubusercontent.com/25688193/41804872-49e5a76c-76d9-11e8-9d47-495ca9d0cd2f.png)<br>

- Linux カーネル<br>
	Linux OS のコアとなる部分で、メモリ管理、ファイルシステム、プロセス管理、デバイス制御などのハードウェアやアプリケーションを制御するための基本的機能を実装したソフトウェア。C 言語やアセンブラ言語で書かれている。<br>
	尚、Andriod は、この Linux カーネル上に構成されている。<br>

- Linux ディストリビューション<br>
	Linux カーネルに加えて、各種コマンド、ライブラリ、アプリケーションを含めたパッケージ。（カーネル以外の部分をユーザーランドという）<br>
	通常 Linux は、Linux ディストリビューションという形でパッケージ化されて配布される。<br>
	
	主要な Linux ディストリビューションは、以下の表のようになる。<br>
	![image](https://user-images.githubusercontent.com/25688193/41805109-a89190f6-76dd-11e8-975d-9fe33ef22196.png)<br>


<a id="ID_4-1"></a>

### ◎ Linux カーネル
Linux カーネルは、以下の代表的な基本機能を持つ。<br>

- デバイス管理<br>
	CPU、メモリ、ディスク、I/O デバイス等のハードウェアをデバイスドライバーで制御する。<br>

- プロセス管理<br>
	Linux は命令を実行する際に、そのプログラムのソースファイルに書かれた内容を読み込み、メモリ上に展開する。その上で、このメモリ上のプログラムを実行するが、この実行されたプログラムを **プロセス** と呼ぶ。OS を動作させたり、各種アプリケーションを実行したりすると、このプロセスが複数同時に実行された状態になる。<br>
	Linux カーネルでは、これらのプロセスに PID（プロセスID）という識別子を付けて管理している。<br>
	そして、プロセス実行のために必要となる CPU を効率よく割り当てる処理を行なっている。<br>

- メモリ管理<br>
	プロセスが起動すると、メモリ上にプログラムが展開されるが、同時にプログラム中で使用するデータもメモリ上に展開されるが、Linux カーネルは、プログラム＆データを物理メモリ上に効率よく割り当て＆解放する機能を持つ。<br>
	
	ここで、メモリには容量の物理的な制限があるが、その物理的な容量を超えるようなプログラム＆データを展開する際には、ハードディスクのような補助記憶装置に仮想メモリ領域を確保し、それらを利用する。（この仮想メモリ領域を **スワップ** と呼ぶ。）<br>
	Linux カーネルは、メモリ上に展開された利用頻度の低いデータをスワップに追い出したり（スワップアウト）、逆に、スワップ上のデータをメモリに戻したり（スワップイン）する機能を持つ。<br>

- シェル [Shell]<br>
	Linux カーネルをユーザーが直接操作するためには、シェル [Shell]（殻）を経由して操作出来る。<br>
	シェルは、ユーザーからの命令をコマンドで受け付け、それを Linux カーネルに伝える役割を持つ。<br>
	具体的には、シェルは以下の事項を実行できる。<br>
	- アプリケーションの起動、停止、再起動<br>
	- 環境変数の管理<br>
	- コマンド履歴の管理（コマンドヒストリ）<br>
	- コマンド実行結果の表示やファイル出力<br>

	又、シェルで実行したい命令をまとめてテキストファイルに記述したものを **シェルスクリプト** という。<br>
	シェルスクリプトでは、制御構文が使用できるため、条件分岐や繰り返し処理を行うことが出来る。<br>
	
	尚、Linux で利用できるシェルには、以下の表のように幾つかの種類が存在する。（シェルの種類が異なれば、シェルスクリプトの書き方も異なる。）<br>
	![image](https://user-images.githubusercontent.com/25688193/41806759-aa5fc7ce-76fe-11e8-8cb7-bfac2ccc1bfe.png)<br>


<a id="ID_4-2"></a>

### ◎ Linux のファイルシステム
Linux ファイルシステムは、Linux でハードディスクや USB メモリ、CD / DVD などデバイスにあるデータにアクセスするための仕組みである。<br>

通常、コンピューターがデータを読み書きする際、ドライバー視点では、データをどのデバイスにどうように保存しているのかを意識した処理を行う必要がある。<br>
一方、データを利用するアプリケーション視点では、データがどのデバイスに保存されていようが、同じ方法で透過的にアクセス出来るのことが望ましい。<br>

Linux カーネルでは、**VFS [Virtual File System]（仮想ファイルシステム）** という仕組みを用いて、このデータへの透過アクセスを可能にしている。<br>
この VFS では、各デバイスをファイルとし取り扱っているのが特徴である。<br>

![image](https://user-images.githubusercontent.com/25688193/41815841-5bb123c6-77b1-11e8-9ef9-de24d041101c.png)<br>

Linux で取り扱う主なファイルシステムは、以下の表のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/41815053-7186a228-779a-11e8-9e6b-c62e4418349d.png)<br>


<a id="ID_4-2-1"></a>

#### ☆ ファイルシステムのマウント
![image](https://user-images.githubusercontent.com/25688193/41815121-eb847a90-779b-11e8-801a-59073b48a692.png)<br>

ディスク上のファイルシステムは、上図のような /（ルートディレクトリ）を頂点とする木構造のどこかのディレクトリに組み込まれている。<br>
逆に言えば、データ（ディスク資源）を利用可能にするためには、この木構造への組み込み（関連付け）作業が必要であり、この作業を **マウント** と呼ぶ。<br>
そして、マウントしたファイルシステムが結合される（組み込まれる）ディレクトリを **マウントポイント** という。（上図では、/home2 ディレクトリがマウントポイント）<br>


<a id="ID_4-3"></a>

### ◎ Linux のディレクトリ構成
Linux は、インストールされると Linux カーネルを含む、各種コマンドや設定ファイルがディレクトリに配置される。<br>
この Linux のディレクトリ一覧は、**FHS [Filesystem Hierarchy Standard]** という規格によって標準化されている。<br>
現在、多くの主要な Linux ディストリビューションが、この FHS 規格を元にディレクトリを構成している。
但し、この FHS 規格に完全準拠しているわけではなく、Linux ディストリビューションによって差異は存在する。<br>

以下の図は、Linux 全体のディレクトリ構成である。<br>
![image](https://user-images.githubusercontent.com/25688193/41815403-ed6cbdbe-77a4-11e8-9a72-9eccaeb1213e.png)<br>

各ディレクトリの概要をもう少し詳しく記載すると、以下の表のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/41816971-e2c5b546-77cc-11e8-8e9b-65e0024d237b.png)<br>


<a id="ID_4-4"></a>

### ◎ Linux のセキュリティ機能
Linux のもつセキュリティ機能の内、代表的なもののみを取り上げる。<br>

- アカウントによる権限設定<br>

- ネットワークフィルタリングによる機能<br>

- SELinux [Security-Enhanced Linux]<br>

---

<a id="ID_5"></a>

## ■ ミドルウェアの基礎事項
開発したアプリケーションを動作させるためには、ハードウェア・ネットワーク・OS の知識に加えて、ミドルウェアの知識も重要になるケースも存在する。<br>

尚、ここでいうミドルウェアとは、OS と業務処理を行うアプリケーションとの中間に位置するソフトウェアのことを指し、OS のもつ機能を拡張したもの、アプリケーションで使用する共通機能を提供するもの、各種サーバー機能を提供するものなど目的や用途に応じて幅広い種類が利用される。<br>
ここでは、Docker を使う上でポイントとなる代表的なミドルウェアの概要を見ていく。<br>

<a id="ID_5-1"></a>

### ◎ Web サーバーのミドルウェア
Web サーバーとは、クライアント（提供される側）となるブラウザ（Web クライアント）からのリクエスト（HTTP リクエスト）を受けて、コンテンツ（HTML や CSS 等）をレスポンスとして返したり、他のサーバーサイドプログラムを呼び出したりする機能を持つサーバーである。<br>

代表的な Web サーバーには、以下の表のようなものが存在する。<br>
![image](https://user-images.githubusercontent.com/25688193/41825615-3c131b6c-785d-11e8-835c-8caf51b141ad.png)<br>

<a id="ID_5-1-1"></a>

#### ☆ サーバーとクライアント
![image](https://user-images.githubusercontent.com/25688193/41824909-37cab4b6-7853-11e8-9a63-ca19a548b05b.png)<br>

- サーバー：何かを提供する人や物<br>
- クライアント：何かを提供される人や物<br>
- Web サーバー：Web サーバーからコンテンツ（HTML や CSS 等）を閲覧できる状態にしてくれる物。<br>
- Web クライアント：Web ブラウザのこと。<br>

以下、サーバー・クライアント間のやり取り。<br>

1. クライアント側がサーバー側にリクエストを投げる。<br>
2. サーバー側でリクエストを解析、処理してリクエストに対するレスポンスを作る。<br>
3. サーバー側がクライアント側にレスポンスを投げる。<br>


<a id="ID_5-2"></a>

### ◎ データベースサーバーのミドルウェア
データベースサーバーは、システムが生成する様々なデータ（データベース）を管理するためのミドルウェアで、**データベース管理システム DBMS [Database Management System]** が動作しているサーバーである。<br>
データの検索、登録、変更、削除の基本機能に加えて、**トランザクション処理** などを含む。<br>

代表的なデータベースには、以下の表のようなものが存在する。<br>
![image](https://user-images.githubusercontent.com/25688193/41831442-fae181ea-7881-11e8-834b-3e9259bc146c.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41832868-f113e7b4-7888-11e8-9726-afc7c0fd63d5.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41832528-6cbb6c36-7887-11e8-933c-fddb76c620e4.png)<br>

![image](https://user-images.githubusercontent.com/25688193/41832552-82210b30-7887-11e8-859e-82c99f29e764.png)<br>

<a id="ID_5-2-1"></a>

#### ☆ NoSQL
NoSQL はリレーショナルデータベース管理システム（RDBMS）とは異なるデータベース管理システム（DBMS）の総称である。<br>
分散並列処理や柔軟な **スキーマ** 設定（データの構造の記述）などを特徴としている。<br>
主な方式としては、KVS（Key-Value ストア）やドキュメント志向データベース、XML データベースなどが存在する。大量データの蓄積や並列処理を得意とするため、多数のユーザーからのアクセスを処理する必要のあるシステム、
例えば、オンラインシステムなどで広く利用されている。<br>

代表的な NoSQL には、以下の表のようなものが存在する。<br>
![image](https://user-images.githubusercontent.com/25688193/41834885-aa3cce84-7890-11e8-9e79-72bf82efb551.png)<br>


<a id="ID_5-3"></a>

### ◎ システム監視ツールのミドルウェア
システムが本番リリースすると、インフラ運用管理業務が開始されるが、この際に、システムを安定駆動させるために、システム管理者はシステムがどのような状態で駆動しているのかを監視する必要性に迫られる。<br>

一般的なシステムでは、これらの監視をシステム監視ツールによって行う。<br>
このシステム監視ツールは、システムの監視対象のサーバーやデバイスの状態を監視し、予め設定した閾値を超えたときに、決められたアクションを実行するツールである。<br>

代表的なシステム監視ツールには、以下の表のようなものが存在する。<br>
![image](https://user-images.githubusercontent.com/25688193/41845954-2ef0e3d0-78b0-11e8-8a14-e2e4859db1e6.png)<br>


---

<a id="ID_6"></a>

## ■ インフラの構成管理の基礎事項
**インフラの構成管理とは、インフラを構成するハードウェア、ネットワーク、OS、ミドルウェア、アプリケーションの構成情報を管理し、適切な状態に保つこと** を意味する。

![image](https://user-images.githubusercontent.com/25688193/41853391-a2f31628-78c8-11e8-876e-8990eff35fd0.png)<br>

オンプレミス環境では、自社で調達した機器を３年・５年・１０年などの提供したベンダーの保守期限が切れるまで使用するので、いったん構築したものをメンテナンスしながら長期間使用していくのが一般的である。<br>
又、機器だけでなく OS やミドルウェアにもベンダーの保守期限が存在し、これらのバージョンアップだけに加えて、本番運用時のトラフィックに合わせながらパフォーマンスのチューニングを行い、インフラの運用管理していくことになる。<br>
この際に、インフラの変更履歴を管理することで運用管理するが、この方法ではインフラの規模が大きくなるほど管理対象が増大してしまう問題があった。<br>

一方、クラウドを利用したインフレの構成管理では、クラウドシステムが仮想環境をベースにしているために、インフラ構成から物理的な制約がなくなり、サーバーやネットワークの構築をインスタンスの生成、破棄という形で手軽に行えるようになる。<br>
そのため、一度構築したインフラは変更を加えることなくインスタンスを破棄して、新しいインスタンスを構築してしまえばよく、これまでオンプレミスの方法では負荷の大きかったインフラの変更履歴を管理する必要がなくなる。そして、その代わりに、動作している **インフラの状態** を管理すればよいことになる。（このようなインフラを **Immutable Infrastructure** という。）<br>

<a id="ID_6-1"></a>

### ◎ コードによるインフラの構成管理
このことをより詳しく見ていく。<br>
オンプレミス環境でのシステム基盤はの多くは、ネットワーク機器をデータセンターに置くが、これらの機器はセットアップを行わないと動作しない。<br>
一方、クラウド環境の場合は、デバイスのキッティング（配備）の工程は不要であるが、代わりに複数のインスタンスセットアップ工程が必要となる。この際に、複数のサーバーのインスタンスを１台づつ手作業で設定していくことは、効率的ではなく、又、作業ミスも発生し得る。このことは、初期のインフラ構築工程だけではなく、システムの本番駆動後の OS やミドルウェアのバージョンアップ、セキュリティパッチの適用の際にも言えることである。<br>

従来のインフラの構成管理（手作業）では、これらの作業をインフラ方式設計書を元にサーバーやネットワーク機器のパラメーターシート（バージョン情報や設定項目に関しての設定値が記されたドキュメント。アプリケーション開発での詳細設計書に相当）を作成し、それをもとにインフラ機器をセットアップしていた。<br>
しかしこの方法では、インフラ構成管理が不十分である場合、プロダクション環境で駆動しているインフラの設計書やサーバーのパラメーターシートが実際の設定値とは異なってしまっている状態が発生してしまい、結果として、環境の構成を変更した際などでうまく動かないケースが発生してし得る。<br>

そのため、最近ではプログラムコードで記述された内容を自動的に設定する仕組みを導入し、そのプログラムを誰が実行しようとも同じ状態のインフラ環境が構築できるようにしている。このようにインフラの構成情報をコードで管理しておけば、アプリケーション開発でのソースコードのバージョン管理と同じように、Git などのバージョン管理ソフトで変更履歴を一元管理することが出来るようになる。<br>
このように、インフラの構成管理を自動化コードで管理していくことを **Infrastructure as Code** と呼ぶ。<br>
**Docker では、Dockerfile という名前のファイルに、インフラの構成管理を記述し、このファイルをの記載を元に Docker イメージを生成することが出来る。** （詳細は後述）<br>

以下の表は、インフラの構成管理を自動化する代表的なツールである。（大まかな分類で、必ずしもこの分類にきっちり当てはまらない自動化ツールも存在する。）<br>
![image](https://user-images.githubusercontent.com/25688193/41905742-01219ba8-7977-11e8-807b-0d63706ab253.png)<br>


<a id="ID_6-2"></a>

### ◎ 継続的インテグレーション、継続的デリバリー
ここでは、コードによりインフラの構成管理が自動化されることにより、アプリケーション全体の開発の流れがどのように変化するのか見ていく。<br>

アプリケーションのコードを追加＆修正する”ごと”にテストを実行して、確実に動作するコードを保持する方法を、**継続的インテグレーション [Continuous Integration]** という。（この継続的インテグレーションは、ソフトウェアの品質向上を目的として考えられた開発プロセスである。）<br>

![image](https://user-images.githubusercontent.com/25688193/41907844-094b0728-797d-11e8-8fe7-1524739c3027.png)<br>

ソフトウェアの特定のパーツについて、仕様書通りに動作するのか確認するテストを単体テストといい、この単体テストを自動化するためのインテグレーションツール（Jenkinsなど）が使用されるが、単体レベルでテスト済みのモジュールが、インフラ環境の異なる環境下でも同じように動作する保証は存在しない。<br>

このようなケースでは、先に見たようにインフラの構成管理をコードで自動化すれば、開発メンバーが常に同じ環境で開発可能になり、継続的インテグレーションで必要となる環境の構成管理が容易になるメリットがある。<br>
<br>

次に、アプリケーションのリリースサイドの流れを見ていく。<br>

![image](https://user-images.githubusercontent.com/25688193/41915582-0713d26e-7991-11e8-8417-451ccc831d50.png)<br>

ウォータフォールモデルでのアプリケーション開発では、要件定義 → 詳細設計書 → コーディング → テストというプロセスを経て、アプリケーション開発が終了した後に、プロダクション環境へアプリケーションをデプロイ（配置）して、サービスのリリースが行われる。<br>
しかしながら、このウォータフォールモデルでの手法では、要件定義からサービスリリースまで長い時間を要するために、サービスリリースした時点で既にアプリケーションが利用者の求めるニーズを満たしていないケースが出てくる。<br>

<br>

![image](https://user-images.githubusercontent.com/25688193/41916449-0513bc70-7993-11e8-9d30-bf0f5bcacc22.png)<br>

これに対し、アジャイル開発では、全ての機能を一度に作るのではなく、機能を追加する度にアプリケーションをプロダクション環境へデプロイし、システム利用者のフィードバックを参考にしながら開発を進めていく。アジャイル開発では開発とリリースを短いサイクルで繰り返すので、利用者の求めるニーズのアプリケーションをタイムリーに提供することが出来る。<br>

---

<a id="ID_7"></a>

## ■ システム基盤の構築、運用の流れ
従来のウォータフォール開発の場合、上流工程から下流工程に順に進めていくことになる。<br>
以下の図は、（インフラ開発における）ウォータフォール開発での手順である。<br>
![image](https://user-images.githubusercontent.com/25688193/41749195-1967c0e6-75f0-11e8-8b3c-3c68949c29c7.png)<br>

一方、アジャイル開発の場合は、迅速かつ適応的に開発を進めていくために、各フェーズでの処理単位を繰り返しながら開発を進めていくことになる。<br>
以下の図は、（インフラ開発における）アジャイル開発での手順である。<br>
![image](https://user-images.githubusercontent.com/25688193/41749981-737b31e0-75f4-11e8-880d-b3497dc83c93.png)<br>

どちらの開発手法の場合でも、（インフラ開発における）ウォータフォール開発やアジャイル開発において、アプリケーション開発とインフラ開発の違いは、**運用フェーズ**の存在となる。<br>
アプリケーション開発においては、プロダクション環境へのシステムリリース後は、バグの修正や追加機能の開発がメインとなり開発要員も減る。
一方でインフラ開発の場合は、リソース監視やセキュリティ対策のためのバージョンアップ、システム障害時対応などの定常業務が残る。<br>
このシステム運用にかかる工程を出来る限り削減し、システムを安定駆動させるために重要なのが**運用設計**となる。具体的には、工程を減らすためにシステム運用で自動化可能な工程を出来る限り自動化するように運用設計することが重要となる。<br>
**Docker は、このシステム構築やシステム運用において、これまで人手で行なっていた作業の多くを自動化し、テスト済みの安全なアプリケーションを継続的に提供することの出来るプラットホームである。**


---

<a id="ID_8"></a>

## ■ Docker

<a id="ID_8-1"></a>

### ◎ Docker のコンテナ技術
Docker では、コンテナ技術を利用してアプリケーションの実行環境を構築＆運用するためのプラットホームである。<br>
ここでは、このコンテナ技術について見ていく。<br>

![image](https://user-images.githubusercontent.com/25688193/41943806-efaf71d8-79df-11e8-833c-c6f60b757f08.png)<br>

**コンテナとは、ホスト OS （仮想マシンを動かしている側のOS）上に論理的な区画（コンテナ）を作り、その中にアプリケーションを動作させるのに必要なライブラリやその他アプリケーションなどを１つにまとめて、あたかも個別のサーバーとして使うことが出来るようにしたものである。**<br>
**ホスト OS のリソースをコンテナ区分から分離し、各コンテナ間で共有して使用する。**（上図）<br>
**コンテナは、オーバヘッド（仮想化を行うために必要となるCPUリソースやディスク容量、メモリ）が少ないので、軽量で高速に動作するのが特徴である。**<br>

<br>

通常、ホスト OS にインストールした（１つのOS上で動作する）通信アプリケーションは、同じシステムリソースを使用する。<br>
このとき、これらの通信アプリケーションは、データを格納するディレクトリを共有し、サーバーに設定されている同じ IP アドレスで通信を行うことになる。そのため、複数のアプリケーションで使用しているミドルウェアやライブラリのバージョンが異なるようなケースでは、お互いのアプリケーションが相互に影響を受けないようにしなくてはならないという問題が生じる。<br>
この問題に対しコンテナ技術を適用すると、OS やディレクトリ、IP アドレスなどのシステムリソースを、個々のアプリケーションがあたかも占有しているように扱えるため、この相互間問題を解決することが出来る。<br>

<a id="ID_8-1-1"></a>

#### ☆ サーバー仮想化技術
コンテナ技術とよく似たものに、サーバー仮想化技術がある。<br>
（但し、これら２つの技術は似ているが目的は異なる。コンテナ技術はアプリケーションの実行環境をまとめることで、アプリケーションの可搬性を高めることを目的としている。一方、サーバー仮想化技術は、異なる環境を効率よくエミュレートすることを目的としている。）<br>

このサーバー仮想化技術は、クラウドの仮想マシンサービスなどで広く使われており、その方式によって、幾つかの種類が存在する。<br>

- ホスト型サーバー仮想化技術<br>
	![image](https://user-images.githubusercontent.com/25688193/41953574-6cd74718-7a11-11e8-92a1-d9e485e37f5a.png)<br>
	ホスト型サーバー仮想化技術は、上図のように、ハードウェア上にベースとなるホスト OS をインストールし、ホスト OS の上に仮想化ソフトをインストールする。そして、その仮想化ソフト上でそれぞれの仮想化環境のゲストOSを動作させる技術である。<br>
	仮想化ソフトをインストールして手軽に仮想環境の構築が行える。<br>
	但し、この方式では、コンテナ技術とは異なり、ホスト OS 上で別のゲスト OS を動作させるので、オーバヘッド（仮想化のためのCPUリソース、ディスク容量、メモリなど）が大きくなるデメリットが存在する。	<br>
	ホスト型サーバー仮想化技術を使用しているツールの代表的な例としては、Oracle が提供している「Oracle VM VirtualBox」や VMware の「VMware Workstation Player」などが存在する。<br>

- ハイパーバイザー型サーバー仮想化技術<br>
	![image](https://user-images.githubusercontent.com/25688193/41958401-e2ef5a54-7a24-11e8-8086-631f76c74c94.png)<br>
	ハイパーバイザー型サーバー仮想化技術は、上図のように、ハードウェア上に仮想化を専用に行うソフトウェアである「ハイパーバイザー」を配置し、ハードウェアと仮想環境を制御する。<br>
	このようにすることで、ホスト OS を利用することなくハードウェアを直接制御することが出来るので、リソースを効率よく使用することが出来るようになる。<br>
	但し、仮想環境ごとに別の OS が動作するので、仮想環境の起動にかかるオーバヘッドは大きくなる。<br>
	ハイパーバイザー型サーバー仮想化技術を使用している代表的なツールとしては、Microsoft Windows Server の「Hyper-V」や Citrix 社の「XenServer」などが存在する。<br>


<a id="ID_8-2"></a>

### ◎ プログラマー視点からの Docker
Web システムの開発において、アプリケーションをプロダクション環境で駆動させるためには、以下のような要素が必要となる。<br>

- アプリケーションの実行モジュール（プログラム本体）<br>
- ミドルウェアやライブラリ群<br>
- OS / ネットワークなどのインフラ環境設定<br>

この際に、Docker を使用しない開発手法では、それぞれの開発フェイズでこれらインフラ環境が必ずしも同じになるとは限らないために、アプリケーションが開発環境やテスト環境では正しく動作しても、ステージング環境（継続的デリバリーが行われるシステム開発において、開発したアプリケーションをプロダクション環境にデプロイする直前に確認するテスト環境）やプロダクション環境にデプロイすると、正常に動作しないケースも有り得る。<br>

**これに対し、Docker では、これらのインフラ環境をコンテナとして管理する。**<br>
**より詳細には、アプリケーションの実行に必要な全てのファイル、ディレクトリ群をコンテナとしてまとめる。そして、このコンテナの雛形になる Docker イメージを Docker Hub などのレポジトリで共有し、この Docker イメージを元に各環境でのコンテナを生成し、各開発フェイズでの同一の環境を実現する。**<br>

以下の図は、Docker を使用したアプリケーション開発の流れを示している。<br>

![image](https://user-images.githubusercontent.com/25688193/42006282-e0b4fc28-7ab3-11e8-9b8b-d3098ac5a442.png)<br>

プログラマー（アプリケーション開発者）は Docker を使用して、開発したアプリケーションの実行に必要な全て（ファイル、ディレクトリ、ライブラリ等）を含んだ Docker イメージを作成する。そして、この作成した Docker イメージを元にコンテナを駆動させる。（上図）<br>

**この Docker イメージは、Docker をインストールしている環境であれば基本的にどこでも動く**ので、先の問題のような「開発・テスト環境では動くけど、プロダクション環境では動かない」といった問題を解決することが出来る。（但し、プロダクション環境では開発環境で必要なライブラリが不要となるケースは存在する。）<br>

そして、アプリケーションの開発からプロダクション環境へのデプロイが、全てアプリケーション開発者の手により行えるようになる。
このことが継続的デリバリーを可能にし、変化に対して強靭なシステム（可搬性の高いシステム）を構築できる。<br>

<br>


<a id="ID_8-3"></a>

### ◎ Docker の機能
Docker には、大きく分けて以下の３つの機能が存在する。

- Build : Docker イメージを作成する機能<br>
- Ship : Docker イメージを共有する機能<br>
- Run : Docker コンテナを動かす機能<br>


<a id="ID_8-3-1"></a>

#### ☆ Bulid : Docker イメージを作る機能
![image](https://user-images.githubusercontent.com/25688193/42010414-7b1fd5f2-7ac9-11e8-8ee9-197f57523fb9.png)<br>

**Docker イメージの正体は、アプリケーションの実行に必要なファイル群が格納されたディレクトリである。**（Docker コマンドを用いると、これらのファイル群を tar 形式ファイルに出力出来る。）<br>
そして、この Docker イメージは、Docker コマンドを用いて手動でイメージファイルを作ることも出来るし、**Dockerfile というインフラ環境の設定ファイルを作っておいて、それを元に自動でイメージファイルを作ることも出来る。**<br>
ただ、**継続的インテグレーション、継続的デリバリーの観点からは、コードによるインフラの構成管理を考えると、後者の Dockerfile を用いて Docker イメージを作成する方法が望ましい。**<br>

**尚、Docker イメージには、１つのイメージに対し１つのアプリケーションのみを入れておき、複数のコンテナを組み合わせてサービスを構築していく手法が推奨されている。**（アプリケーションの可搬性向上のため）<br>

<br>
<br>

更に、**Docker イメージは積み重ねて使用することも出来る。**<br>
例えば、下図のように、OS 用のイメージに Web アプリケーションサーバー用のイメージを重ねて、別の新しいイメージを作成することが出来る。<br>
その際に、**Docker では構成に変更のあった部分を差分（イメージレイヤー）として管理する。**<br>

![image](https://user-images.githubusercontent.com/25688193/42011489-9ee6d79c-7ace-11e8-80ce-2a569df5ede1.png)<br>

<br>

<a id="ID_8-3-2"></a>

#### ☆ Ship : Docker イメージを共有する機能
Docker イメージは、Docker レジストリで共有できる。<br>
例えば、公式の Docker レジストリである Docker Hub では、Ubuntu や CentOS などの Linux ディストリビューションの基本機能を提供するベースイメージが配布されている。実際のイメージの作成では、これらのベースイメージにミドルウェアやライブラリ、デプロイするアプリケーションなどを加えたイメージを積み重ねて独自の新たな Docker イメージを作成することになる。<br>
尚、Docker Hub には、公式のイメージ以外にも、個人で作成したイメージが配布されており、必要に応じてそれらをダウンロードして使用することが出来る。<br>

又、Docker Hub は、GitHub と連携することも出来る。<br>
即ち、**GitHub 上で Dockerfile を管理し、そこにある Dockerfile から Docker イメージを自動生成し、DockerHub で公開することが出来る。（Automated Bulid）**<br>

<br>

<a id="ID_8-3-3"></a>

#### ☆ Run : Docker コンテナを動かす機能
Docker は Linux 上でコンテナ単位でサーバー機能を動かしている。<br>
**コンテナの起動・停止・破棄は、Docker のコマンドで行う。**<br>

コンテナの起動にあたって、他の仮想化技術でサーバーを起動する際は OS を起動させるところから始めるので、プロセスの起動に時間がかかるが、Docker では、既に動いている OS（ホストOS）上で（コンテナ内で動作する）プロセスを起動するので、高速に起動させることが出来る。<br>

<br>

又、**Docker では、１つの Linux カーネルを複数の コンテナで共有しているが、これらコンテナ内で動作しているプロセスを１つのグループとして管理している。**<br>
**具体的には、グループごとにそれぞれのファイルシステムやホスト名、ネットワークなどを割り当て、グループが異なる場合はプロセスやファイルへのアクセスが出来なくなっている。**<br>

これにより、グループ化された各コンテナを独立した空間として管理出来る。<br>
そして、これらの**グループ機能を実現するために、Linux カーネル機能（namespace, cgroups など）の技術を使用している。**<br>

<br>

<a id="ID_8-4"></a>

### ◎ Docker を構成するコンポーネント
Docker は、以下の図のようないくつかのコンポーネントから構成される。<br>
**Docker では、コア機能である Docker Engine を中心に、コンポーネントを組み合わせてアプリケーションの実行環境を構築する。**<br>

![image](https://user-images.githubusercontent.com/25688193/42015989-aa165906-7ae4-11e8-91f1-ad6654542e10.png)<br>


<a id="ID_8-4-1"></a>

#### ☆ Docker Engine（Docker のコア機能）
Docker イメージの生成やコンテナの起動を行うための Docker のコア機能となるコンポーネント。<br>
Docker コマンドの実行や Dockerfile によるイメージ生成も行う。<br>

<a id="ID_8-4-2"></a>

#### ☆ Docker Registry（イメージ公開・共有）
Docker イメージを公開・共有するためのレジストリ機能となるコンポーネント。<br>
Docker Hub もこの Docker Resistry コンポーネントを使用している。<br>

<a id="ID_8-4-3"></a>

#### ☆ Docker Compose（複数コンテナ一元管理）
複数のコンテナの構成情報をコードで定義して、コマンドを実行することで、<br>
アプリケーションの実行環境を構成するコンテナ群を一元管理するためのツール。<br>

<a id="ID_8-4-4"></a>

#### ☆ Docker Machine（Docker 実行環境構築）
クラウド環境（ローカルホスト用の VirtualBox や Amazon Web Service EC2 , Microsoft Azura など）などに Docker の実行環境をコマンドで自動生成するためのツール。<br>

<a id="ID_8-4-5"></a>

#### ☆ Docker Swarm（クラスタ管理）
複数の Docker ホスト（IPアドレスを割り振られたネットワークノード）をクラスタ化するためのツール。<br>
Docker Swarm では、クラスタの管理や API の提供を行う役割が Manager となり、Docker コンテナを実行する役割が Node となる。<br>

<br>


<a id="ID_8-5"></a>

### ◎ Docker のコア機能が動く仕組み
ここでは、Docker のコア機能がどうのような仕組みで動いているのか、もう少し詳しく見ていく。<br>

<a id="ID_8-5-1"></a>

#### ☆ コンテナを区画化する仕組み（namespace）
![image](https://user-images.githubusercontent.com/25688193/42095100-a23e4afa-7bec-11e8-8588-85333e11449d.png)<br>

**Docker では、コンテナを区画する技術として、Linux カーネルの機能の１つである namespace（名前空間）という機能を使用している。**<br>

Linux OS では、OS 起動時にデフォルトの名前空間が存在し、デフォルトでは全てのプロセスがその名前空間に属する。プロセスの起動時に独立した名前空間でプロセスを実行する指定を行うと、そのプロセスは別の名前空間で実行される。どのリソースを独立・隔離させたいかに応じて、以下の６つの独立した環境を構築できる。<br>

**Docker では、これらの Linux カーネルでの namespace 機能を使用して、ホスト OS 上での各コンテナの仮想的な隔離化を実現している。**（※この仕組みは、Docker を使う上では特に意識する必要はないが、Docker の仕組みを理解するのに役立つ。）<br>

![image](https://user-images.githubusercontent.com/25688193/42083118-e88c0452-7bc4-11e8-8463-fb5a536895b7.png)<br>


<a id="ID_8-5-2"></a>

#### ☆ リソース管理の仕組み（cgroups）
![image](https://user-images.githubusercontent.com/25688193/42095254-0c37e984-7bed-11e8-9976-554ce8e87327.png)<br>

**Docker では、物理マシン上のリソースを複数のコンテナで共有して動作しているが、このときのリソースの割り当てなどの管理は、Linux カーネルの機能の１つである「control groups (cgroups)」機能を使用している。**<br>

Linux では、プログラムをプロセスとして実行するが、このプロセスは１つ以上のスレッドの塊として動作する。<br>
cgroups は、このプロセス・スレッドをグループ化して、そのグループ内に存在するプロセス・スレッドに対して管理を行うための機能である。<br>
この機能を使えば、例えば、ホストOSのCPUやメモリなどのリソースに対し、cgroups でグループ化されたグループ毎に制限をかけることが出来る。<br>

**Docker では、この cgroups 機能を利用して、各コンテナ内のプロセスに対しリソース制限を課すことで、例えば、あるコンテナがホストOSのリソースを使い尽くして、同じホスト OS 上で駆動して他のコンテナに影響を与えてしまうといった事態を防いでいる。**<br>

<br>

cgroups 機能で管理できる代表的なリソースは、以下の表のようになる。<br>
![image](https://user-images.githubusercontent.com/25688193/42095621-10f3a85e-7bee-11e8-933f-0b2d67888fcf.png)<br>

<br>

cgroups は、階層構造を使ってプロセスをグループ化して管理出来る。（下図）<br>

![image](https://user-images.githubusercontent.com/25688193/42097179-6bceff68-7bf2-11e8-9f41-b898cac8467a.png)<br>

例えば、上図のように、ユーザーアプリケーションのプロセスとデーモンプロセス（バックグラウンドで動作するプロセス）を階層構造で分割し、それぞれのグループに CPU 使用量を割り当てるといった管理が可能である。<br>
このとき、cgroups の階層構造での親子関係では、子は親の制限を受けるために、例え子が親の制限を超えるような設定を行っても、親の cgroups 制限にひっかかる動作となる。（上図では、子の CPU 使用量の総和は、親の CPU 使用量を超えることは出来ない）<br>

<br>

<a id="ID_8-5-3"></a>

#### ☆ ネットワーク構成（仮想ブリッジ・仮想 NIC）
![image](https://user-images.githubusercontent.com/25688193/42121986-5905f318-7c75-11e8-8fef-8f2ffcfd8f47.png)<br>

**Docker コンテナが外部ネットワークと通信を行う際には、仮想ブリッジとホスト OS の物理 NIC 間でパケット転送を行える仕組みが必要である。**<br>
**これを実現するために、Docker のネットワークは、上図のような構成を持っている。**<br>

- Linux に Docker を接続すると、サーバーの物理 NIC が docker0 という仮想ブリッジに接続される。<br>
	（この docker 0 は、Docker 起動後にデフォルトで作成される仮想ブリッジである。）<br>

- コンテナ内の eth0 には、ホスト OS 側で作成された仮想NIC（vethxxx）がペアで割り当てられ、
	仮想 NIC を通じて仮想ブリッジ docker0 に接続される。<br>
	（仮想 NIC は、OSI 参照モデルの第２層：データリンク層に対応するデバイスで、ペアの NIC 同士でトンネリング通信を行う。）

- Docker のコンテナを起動すると、コンテナに 172.17.0.0/16 のサブネットマスクを持つプライベート IP アドレスが、eth0（物理NIC）に自動的に割り当てられる。<br>
	（コンテナ内の eth0 には、空いている IP アドレスが自動的に割り当てられる。）<br>

- Docker では、NAPT の機能を使用して、コンテナと外部ネットワークとの通信を実現する。<br>
	Docker で、NAPT の機能を使用するときは、コンテナの起動時に、コンテナ内で使用しているポートを仮想ブリッジ docker0 に対して開放する。<br>
	例えば、コンテナの起動時に、コンテナ内の Web サーバーが使用する 80 番ポートを、ホスト OS の 8080 番ポートに転送するように設定（開放）する。<br>
	そうすると、外部ネットワークからホスト OS の 8080 番ポートにアクセスすることで、コンテナ内の 80 番ポートに接続出来る。<br>

![image](https://user-images.githubusercontent.com/25688193/42120445-029b875a-7c56-11e8-8fa5-5b82918257d1.png)<br>

![image](https://user-images.githubusercontent.com/25688193/42120453-16b93b6a-7c56-11e8-98a6-ea3e44ea748b.png)<br>

![image](https://user-images.githubusercontent.com/25688193/42120460-2d005886-7c56-11e8-9b29-9eee5e0d5807.png)<br>

##### 【補足】NAT と NAPT（IPマスカレード）
プライベート IP アドレス（ローカルネットワークに接続するのに必要なIPアドレス）とグローバル IP アドレス（インタネットに接続するのに必要な IP アドレス）を変換して、プライベート IP アドレスが割り当てられたコンピューターに対し、インタネットアクセスを可能にする技術として、NAT と NAPT が存在する。<br>

- NAT [Network Adress Translation]<br>
	![image](https://user-images.githubusercontent.com/25688193/42121474-d3f17fda-7c6a-11e8-9eb2-230937cb3396.png)<br>
	プライベートネットワーク上のコンピューター（クライアント）が、インタネット上にあるサーバーにアクセスする際に、NAT ルーターは、クライアントのプライベート IP アドレス（例：192.168.0.15）を、NAT が保持しているグローバルIPアドレス（例：198.51.100.20）に変換してリクエストを送信する。<br>
	レスポンスでは、NAT ルータが宛先をクライアントのプライベート IP アドレスに変換して送信する。<br>
	このアドレス変換により、プライベートネットワーク上のコンピューターとインタネット上のサーバーとの通信が実現するが、NAT の場合は、プライベート IP アドレスとグローバル IP アドレスを１対１で変換するために、同時に複数のクライアントがインタネットにアクセスすることはできない。<br>

- NAPT [Network Adress Port Translation]<br>
	![image](https://user-images.githubusercontent.com/25688193/42121915-8fad686c-7c73-11e8-9d4d-86502f463aba.png)<br>
	NAPT は、プライベート IP アドレスをグローバル IP アドレスに変換する際に、ポート番号も加えて変換する。<br>
	このとき、プライベート IP アドレス毎に異なるポート番号に変換する。<br>
	これにより、例えば、クライアント A からのリクエストはポート番号 100、クライアント B からのリクエストはポート番号 200 に変換した場合、インタネットサーバーからは、NAPT のグローバル IP アドレスの異なるポート番号宛にレスポンスが戻ってきて、NAPT は異なるポート番号を元に、複数のプライベート IP アドレスに変換する事ができる。<br>
	よって、１つのグローバル IP アドレスと複数のプライベート IP アドレスを変換して出来るようになり、
	同時に複数のクライアントがインタネットに接続できるようになる。<br>
	尚、NAPT は技術の名称で、IP マスカレードは Linux における NAPT の実装のことを指す。<br>

<br>

<a id="ID_8-5-4"></a>

#### ☆ Docker イメージのデータ管理の仕組み
